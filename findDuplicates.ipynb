{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqeZku+86mMOcGqXGn9xqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcory-hub/datasettools/blob/main/findDuplicates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8UYMKKKob-m"
      },
      "source": [
        "# Delete duplicate images\n",
        "\n",
        "Last accessed: 2025-12-15\n",
        "\n",
        "## 1. Compile and transfere your images from your local computer to colab\n",
        "\n",
        "1. On your local computer make a folder `images`.\n",
        "2. Put all images from one insect type in the folder `images`.\n",
        "3. Compile the folder to `images.zip`. On mac use `zip -r dataset.zip images -i '*.jpg' '*.txt' '*/'` to exclude macOS metadate from the zipped file.\n",
        "4. Put the zipped images in you google drive\n",
        "5. Continue with the next code blocks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABFb-xklSl-N"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Unzip file with images\n",
        "!mkdir '/content/images'\n",
        "!scp '/content/gdrive/MyDrive/images.zip' '/content/images.zip'\n",
        "!unzip '/content/images.zip'\n",
        "\n",
        "# Optional: Check is path is correct\n",
        "!ls '/content/images/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ImageHash\n",
        "!pip install Pillow"
      ],
      "metadata": {
        "id": "ljmPzr3fM8Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Comparison of the images\n",
        "\n",
        "- Default threshold: 0.80\n",
        "- Check for real duplicates, and adjust the threshold in the next coding block to remove true duplicates and look-alikes.\n",
        "\n"
      ],
      "metadata": {
        "id": "dCl6_SWrc6bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precomputes hashes for all images: Uses the Discrete Cosine Transform (DCT) to focus on the low-frequency components of the image.\n",
        "# Finds similar image pairs.\n",
        "# Sorts and displays pairs with similarity scores above a threshold (0.85).\n",
        "# Saves results to the output_file.\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "from concurrent.futures import ProcessPoolExecutor  # For parallel processing\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imagehash  # For perceptual hashing of images\n",
        "from tqdm import tqdm  # For progress visualization\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow  # For displaying images in Google Colab\n",
        "\n",
        "# Define output folder and file for saving results\n",
        "output_folder = '/content/images'  # Directory where images are stored\n",
        "output_file = 'similar_images.txt'  # File to store results of similar image pairs\n",
        "\n",
        "# Function to compute perceptual hashes for an image\n",
        "def compute_image_hash(image_path, hash_sizes=[8, 16, 32]):\n",
        "    try:\n",
        "        # Open image file and convert to grayscale\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert(\"L\")\n",
        "            hashes = []\n",
        "            # Compute hashes for each specified size\n",
        "            for size in hash_sizes:\n",
        "                resized = img.resize((size * 4, size * 4), Image.LANCZOS)\n",
        "                hashes.append(str(imagehash.phash(resized, hash_size=size)))\n",
        "            return hashes\n",
        "    except Exception:\n",
        "        # Handle errors during hash computation\n",
        "        return None\n",
        "\n",
        "# Function to precompute hashes for all images\n",
        "def precompute_hashes(image_files):\n",
        "    hashes = {}\n",
        "    # Compute hash for each image\n",
        "    for img_file in image_files:\n",
        "        img_path = os.path.join(output_folder, img_file)\n",
        "        img_hash = compute_image_hash(img_path)\n",
        "        if img_hash:\n",
        "            hashes[img_file] = img_hash\n",
        "    return hashes\n",
        "\n",
        "# Function to compare hashes in a given chunk of data\n",
        "def compare_hashes_chunk(chunk):\n",
        "    similar_pairs = []\n",
        "    # Iterate through pairs of images in the chunk\n",
        "    for i in range(len(chunk)):\n",
        "        for j in range(i + 1, len(chunk)):\n",
        "            img1, hashes1 = chunk[i]\n",
        "            img2, hashes2 = chunk[j]\n",
        "            if hashes1 is not None and hashes2 is not None:\n",
        "                # Calculate similarity for each hash size\n",
        "                similarities = [1 - (int(h1, 16) ^ int(h2, 16)).bit_count() / (len(h1) * 4)\n",
        "                                for h1, h2 in zip(hashes1, hashes2)]\n",
        "                max_similarity = max(similarities)\n",
        "                # Check if similarity exceeds threshold\n",
        "                if max_similarity > 0.80:  # Threshold for similarity\n",
        "                    similar_pairs.append((max_similarity, (img1, img2)))\n",
        "    return similar_pairs\n",
        "\n",
        "# Function to find all similar image pairs based on precomputed hashes\n",
        "def find_similar_pairs(hashes):\n",
        "    hash_items = list(hashes.items())\n",
        "    # Divide data into chunks for parallel processing\n",
        "    chunk_size = max(1, len(hash_items) // os.cpu_count())\n",
        "    chunks = [hash_items[i:i + chunk_size] for i in range(0, len(hash_items), chunk_size)]\n",
        "\n",
        "    # Use ProcessPoolExecutor to parallelize hash comparisons\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(compare_hashes_chunk, chunks), total=len(chunks), desc=\"Comparing hashes\"))\n",
        "\n",
        "    # Flatten results from all chunks\n",
        "    return [item for sublist in results for item in sublist]\n",
        "\n",
        "# Function to display an image with its filename using OpenCV\n",
        "def display_image_with_filename(image_path):\n",
        "    try:\n",
        "        # Load and resize the image for display\n",
        "        img = cv2.imread(image_path)\n",
        "        img_resized = cv2.resize(img, (64, 64))\n",
        "        filename = os.path.basename(image_path)\n",
        "\n",
        "        # Create an overlay with the filename\n",
        "        text_img = np.zeros((20, 64, 3), dtype=np.uint8)\n",
        "        cv2.putText(text_img, filename, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
        "\n",
        "        # Combine image and overlay\n",
        "        combined_img = np.vstack((img_resized, text_img))\n",
        "\n",
        "        # Display the image in Colab\n",
        "        cv2_imshow(combined_img)\n",
        "        print(f\"Filename: {filename}\")\n",
        "    except Exception as e:\n",
        "        # Handle errors during image display\n",
        "        print(f\"Error displaying image {image_path}: {e}\")\n",
        "\n",
        "# Function to process a list of images and find similar pairs\n",
        "def process_images(image_list):\n",
        "    start_time = time.time()  # Record start time\n",
        "\n",
        "    # Precompute hashes for all images\n",
        "    print(\"Precomputing hashes...\")\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        chunk_size = max(1, len(image_list) // os.cpu_count())\n",
        "        chunks = [image_list[i:i + chunk_size] for i in range(0, len(image_list), chunk_size)]\n",
        "        results = list(tqdm(executor.map(precompute_hashes, chunks), total=len(chunks), desc=\"Computing hashes\"))\n",
        "\n",
        "    # Combine results into a single dictionary\n",
        "    image_hashes = {k: v for d in results for k, v in d.items()}\n",
        "\n",
        "    # Find pairs of similar images\n",
        "    print(\"Finding similar pairs...\")\n",
        "    similar_pairs = find_similar_pairs(image_hashes)\n",
        "\n",
        "    # Sort similar pairs by similarity score\n",
        "    similar_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # Display and save similar pairs\n",
        "    print(\"\\nSimilar pairs above 0.85 threshold:\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        for i, (similarity, (img1_name, img2_name)) in enumerate(similar_pairs):\n",
        "            print(f\"\\nPair {i+1} with similarity: {similarity:.4f}\")\n",
        "            f.write(f\"Pair {i+1} with similarity: {similarity:.4f}\\n\")\n",
        "\n",
        "            img1_path = os.path.join(output_folder, img1_name)\n",
        "            img2_path = os.path.join(output_folder, img2_name)\n",
        "\n",
        "            print(\"Image 1:\")\n",
        "            print(f\"Filename: {img1_name}\")\n",
        "            f.write(f\"Image 1: {img1_name}\\n\")\n",
        "            display_image_with_filename(img1_path)\n",
        "\n",
        "            print(\"\\nImage 2:\")\n",
        "            print(f\"Filename: {img2_name}\")\n",
        "            f.write(f\"Image 2: {img2_name}\\n\\n\")\n",
        "            display_image_with_filename(img2_path)\n",
        "\n",
        "    # Display summary and execution time\n",
        "    print(f\"\\nTotal similar pairs found: {len(similar_pairs)}\")\n",
        "    print(f\"Results have been saved to {output_file}\")\n",
        "    end_time = time.time()\n",
        "    print(f\"Comparison complete! Time taken: {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# Main function to initiate image processing\n",
        "def main():\n",
        "    print(\"Processing all images in the folder...\")\n",
        "    all_images = os.listdir(output_folder)  # Get list of all images in the output folder\n",
        "    process_images(all_images)  # Process images for similarity\n",
        "\n",
        "# Run the main function if the script is executed\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "9nmwIP1BgCGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Remove duplicate images from folder images\n",
        "\n",
        "Find the lowest similarity where the images are real duplicaties and adjust code below. The images with the longest name are removed."
      ],
      "metadata": {
        "id": "DMeDTfs8T_FC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qI4xL-2XSgjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import imagehash\n",
        "from PIL import Image\n",
        "\n",
        "# Output folder path (change as needed)\n",
        "output_folder = '/content/images'\n",
        "\n",
        "def compute_image_hash(image_path):\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert(\"L\").resize((8, 8), Image.LANCZOS)  # Resize for hash computation\n",
        "            return imagehash.phash(img) # Return the hash object directly\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def find_and_remove_duplicates(folder, similarity_threshold=0.8100):\n",
        "    all_image_paths = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if not filename.startswith('.'):  # Skip hidden files\n",
        "            file_path = os.path.join(folder, filename)\n",
        "            if os.path.isfile(file_path):\n",
        "                all_image_paths.append(file_path)\n",
        "\n",
        "    # Store filepath -> hash_object mapping for all files\n",
        "    file_hashes = {}\n",
        "    for img_path in all_image_paths:\n",
        "        file_hashes[img_path] = compute_image_hash(img_path)\n",
        "\n",
        "    # Keep track of files that have been successfully removed\n",
        "    removed_files_set = set()\n",
        "    total_removed = 0\n",
        "\n",
        "    # Iterate through all unique pairs of images\n",
        "    num_images = len(all_image_paths)\n",
        "    for i in range(num_images):\n",
        "        for j in range(i + 1, num_images):\n",
        "            path1 = all_image_paths[i]\n",
        "            path2 = all_image_paths[j]\n",
        "\n",
        "            # Skip if either file has already been removed\n",
        "            if path1 in removed_files_set or path2 in removed_files_set:\n",
        "                continue\n",
        "\n",
        "            hash_obj1 = file_hashes.get(path1)\n",
        "            hash_obj2 = file_hashes.get(path2)\n",
        "\n",
        "            if hash_obj1 is None or hash_obj2 is None:\n",
        "                continue\n",
        "\n",
        "            distance = hash_obj1 - hash_obj2\n",
        "            # For 8x8 pHash, hash.size is 64\n",
        "            similarity = 1 - distance / (hash_obj1.hash.size)\n",
        "\n",
        "            if similarity >= similarity_threshold:\n",
        "                fname1 = os.path.basename(path1)\n",
        "                fname2 = os.path.basename(path2)\n",
        "\n",
        "                # Determine which file to remove (the one with the longer name, or lexicographically larger if names are equal)\n",
        "                to_remove_path = None\n",
        "                if len(fname1) > len(fname2):\n",
        "                    to_remove_path = path1\n",
        "                elif len(fname2) > len(fname1):\n",
        "                    to_remove_path = path2\n",
        "                else:\n",
        "                    to_remove_path = max(path1, path2) # Lexicographically larger path\n",
        "\n",
        "                if to_remove_path and to_remove_path not in removed_files_set:\n",
        "                    print(f\"Removing duplicate image: {os.path.basename(to_remove_path)}\")\n",
        "                    os.remove(to_remove_path)\n",
        "                    removed_files_set.add(to_remove_path)\n",
        "                    total_removed += 1\n",
        "\n",
        "    print(f\"\\nTotal number of duplicate images removed: {total_removed}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Call the function with the desired similarity threshold\n",
        "    find_and_remove_duplicates(output_folder, similarity_threshold=0.8100)\n"
      ],
      "metadata": {
        "id": "I_y3XNM6SY6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the file\n",
        "Download `images_noduplicates.zip` before closing colab!"
      ],
      "metadata": {
        "id": "h7F9M7Wqn903"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Define the folder containing images and the name of the zip file\n",
        "output_folder = '/content/images'\n",
        "zip_file_name = '/content/images_noduplicates.zip'  # Specify the path for the zip file\n",
        "\n",
        "def zip_images(folder, zip_file):\n",
        "    with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
        "        # Add all images in the folder to the zip file\n",
        "        for filename in os.listdir(folder):\n",
        "            file_path = os.path.join(folder, filename)\n",
        "            if os.path.isfile(file_path):\n",
        "                zipf.write(file_path, arcname=filename)\n",
        "    print(f\"Created zip file: {zip_file}\")\n",
        "\n",
        "# Call the function to zip images\n",
        "zip_images(output_folder, zip_file_name)"
      ],
      "metadata": {
        "id": "6ZeWA1IQVdsE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}